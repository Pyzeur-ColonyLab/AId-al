#!/usr/bin/env python3
"""Fixed Mistral AI Chatbot with Token Support"""
import os
import logging
import torch
from telegram.ext import Application, CommandHandler, MessageHandler, filters
from huggingface_hub import login

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global variables
model = None
tokenizer = None
model_name = os.getenv('MODEL_NAME', 'Pyzeur/Code-du-Travail-mistral-finetune')

def load_model():
    global model, tokenizer
    try:
        logger.info(f"üîÑ Loading Mistral model: {model_name}")
        
        # Login to HuggingFace if token provided
        hf_token = os.getenv('HF_TOKEN')
        if hf_token:
            logger.info("üîë Logging in to HuggingFace...")
            login(token=hf_token)
        
        # Method 1: Try with transformers pipeline
        try:
            from transformers import pipeline
            logger.info("üìù Attempting pipeline approach...")
            
            model = pipeline(
                "text-generation",
                model=model_name,
                device_map="cpu",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                token=hf_token
            )
            logger.info("‚úÖ Pipeline approach successful!")
            return True
            
        except Exception as e1:
            logger.error(f"‚ùå Pipeline failed: {e1}")
            
            # Method 2: Manual loading
            try:
                from transformers import AutoModelForCausalLM, AutoTokenizer
                logger.info("üîß Attempting manual loading...")
                
                # Load tokenizer
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    token=hf_token,
                    trust_remote_code=True
                )
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                # Load model
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    device_map="cpu",
                    trust_remote_code=True,
                    token=hf_token,
                    low_cpu_mem_usage=True
                )
                logger.info("‚úÖ Manual loading successful!")
                return True
                
            except Exception as e2:
                logger.error(f"‚ùå Manual loading failed: {e2}")
                
                # Method 3: Fallback to base model
                try:
                    logger.info("üîÑ Falling back to base Mistral model...")
                    from transformers import pipeline
                    model = pipeline(
                        "text-generation",
                        model="mistralai/Mistral-7B-Instruct-v0.1",
                        device_map="cpu",
                        torch_dtype=torch.float16
                    )
                    logger.info("‚úÖ Base model fallback successful!")
                    return True
                    
                except Exception as e3:
                    logger.error(f"‚ùå All methods failed: {e3}")
                    raise Exception("Could not load any model")
                    
    except Exception as e:
        logger.error(f"‚ùå Fatal error loading model: {e}")
        raise

def generate_response(user_input):
    global model, tokenizer
    try:
        if hasattr(model, '__call__'):  # Pipeline approach
            prompt = f"<s>[INST] {user_input} [/INST]"
            result = model(
                prompt,
                max_new_tokens=150,
                temperature=0.7,
                do_sample=True,
                return_full_text=False
            )
            response = result[0]['generated_text'].strip()
            
        else:  # Manual approach
            prompt = f"<s>[INST] {user_input} [/INST]"
            inputs = tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            if prompt in response:
                response = response.replace(prompt, "").strip()
        
        return response or "Je ne peux pas r√©pondre √† cette question."
        
    except Exception as e:
        logger.error(f"‚ùå Generation error: {e}")
        return "D√©sol√©, une erreur s'est produite lors de la g√©n√©ration."

async def start_command(update, context):
    await update.message.reply_text(
        "ü§ñ **Assistant Juridique IA**\n\n"
        "‚öñÔ∏è **Sp√©cialis√© en Droit du Travail Fran√ßais**\n"
        f"üß† **Mod√®le**: {model_name.split('/')[-1]}\n\n"
        "üíº **Expertise:**\n"
        "‚Ä¢ Code du Travail fran√ßais\n"
        "‚Ä¢ Licenciements et ruptures\n"
        "‚Ä¢ Cong√©s et temps de travail\n"
        "‚Ä¢ Contrats et r√©mun√©rations\n\n"
        "‚ùì **Posez votre question juridique!**"
    )

async def info_command(update, context):
    info_text = f"""
ü§ñ **Informations du Syst√®me**
- **Mod√®le IA**: {model_name.split('/')[-1]}
- **Base**: Mistral 7B Fine-tun√©
- **Sp√©cialit√©**: Droit du Travail FR
- **Statut**: ‚úÖ Op√©rationnel
- **Type**: Intelligence Artificielle
"""
    await update.message.reply_text(info_text)

async def chat_message(update, context):
    try:
        user_message = update.message.text
        logger.info(f"üë§ Question: {user_message}")
        
        # Show typing
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action="typing")
        
        # Generate AI response
        response = generate_response(user_message)
        
        logger.info(f"ü§ñ R√©ponse IA: {response}")
        await update.message.reply_text(response)
        
    except Exception as e:
        logger.error(f"‚ùå Chat error: {e}")
        await update.message.reply_text(
            "‚ö†Ô∏è Erreur technique. Veuillez r√©essayer."
        )

def main():
    try:
        logger.info("üöÄ Initializing Mistral Legal AI...")
        
        # Load model
        load_model()
        
        # Create bot
        app = Application.builder().token(os.getenv('TELEGRAM_BOT_TOKEN')).build()
        
        # Add handlers
        app.add_handler(CommandHandler("start", start_command))
        app.add_handler(CommandHandler("info", info_command))
        app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, chat_message))
        
        logger.info("‚úÖ Mistral Legal AI ready!")
        logger.info("üì± Starting Telegram service...")
        
        # Run bot
        app.run_polling()
        
    except Exception as e:
        logger.error(f"‚ùå Startup failed: {e}")
        logger.info("üîÑ Container staying alive...")
        import time
        while True:
            time.sleep(60)

if __name__ == "__main__":
    main()
