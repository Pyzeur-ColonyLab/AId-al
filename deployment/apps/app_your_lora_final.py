#!/usr/bin/env python3
"""Final Optimized YOUR LoRA Model"""
import os
import logging
import torch
from telegram.ext import Application, CommandHandler, MessageHandler, filters
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from huggingface_hub import login
import gc
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

model = None
tokenizer = None
model_loaded = False
loading_progress = "Initialisation..."

def load_your_lora_model():
    global model, tokenizer, model_loaded, loading_progress
    try:
        # Login
        hf_token = os.getenv('HF_TOKEN')
        if hf_token:
            login(token=hf_token)
        
        logger.info("ğŸ¯ Loading YOUR LoRA model (memory optimized)...")
        loading_progress = "ğŸ”„ Connexion Ã  HuggingFace..."
        
        # Aggressive memory cleanup
        gc.collect()
        
        # Load base model with maximum memory optimization
        base_model_name = "mistralai/Mistral-7B-Instruct-v0.3"
        loading_progress = "ğŸ“¥ TÃ©lÃ©chargement du modÃ¨le de base..."
        logger.info(f"ğŸ“¥ Loading base model with max optimization...")
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="cpu",
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            max_memory={"cpu": "7GB"},  # Limit memory usage
            offload_folder="./tmp"
        )
        
        loading_progress = "âœ… ModÃ¨le de base chargÃ©!"
        logger.info("âœ… Base model loaded!")
        
        # Small delay to let memory settle
        time.sleep(2)
        gc.collect()
        
        # Load tokenizer
        loading_progress = "ğŸ“ Chargement du tokenizer..."
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        loading_progress = "ğŸ”§ Application de VOS adaptateurs LoRA..."
        logger.info("ğŸ“ Tokenizer loaded!")
        
        # Apply YOUR LoRA adapters
        lora_model_name = "Pyzeur/Code-du-Travail-mistral-finetune"
        logger.info(f"ğŸ”§ Applying YOUR LoRA adapters...")
        
        model = PeftModel.from_pretrained(
            base_model,
            lora_model_name,
            token=hf_token
        )
        
        model_loaded = True
        loading_progress = "âœ… VOTRE modÃ¨le est prÃªt!"
        logger.info("âœ… YOUR LoRA model loaded successfully!")
        logger.info("ğŸ›ï¸ Code du Travail expertise fully active!")
        
    except Exception as e:
        loading_progress = f"âŒ Erreur: {str(e)[:50]}..."
        logger.error(f"âŒ Error: {e}")
        logger.info("ğŸ”„ Keeping bot alive despite model error...")

def generate_response(user_question):
    global loading_progress
    
    if not model_loaded:
        return f"â³ **Chargement de VOTRE modÃ¨le personnalisÃ©...**\n\nğŸ“Š **Progression**: {loading_progress}\n\nğŸ¯ **Votre modÃ¨le**: Code-du-Travail-mistral-finetune\nâš–ï¸ **SpÃ©cialitÃ©**: Droit du travail franÃ§ais\n\nâ° Patientez quelques minutes, votre expertise arrive!"
    
    try:
        prompt = f"<s>[INST] {user_question} [/INST]"
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(prompt, "").strip()
        
        return f"ğŸ¤– **[VOTRE MODÃˆLE PERSONNEL]**\n\n{response}\n\n---\nğŸ¯ *RÃ©ponse gÃ©nÃ©rÃ©e par votre modÃ¨le Code-du-Travail-mistral-finetune*"
        
    except Exception as e:
        logger.error(f"Generation error: {e}")
        return "âŒ Erreur lors de la gÃ©nÃ©ration avec votre modÃ¨le."

async def start_command(update, context):
    status_emoji = "âœ…" if model_loaded else "â³"
    status_text = "OpÃ©rationnel" if model_loaded else "En cours de chargement"
    
    await update.message.reply_text(
        f"ğŸ¤– **VOTRE Assistant Juridique Personnel**\n\n"
        f"ğŸ¯ **ModÃ¨le**: Code-du-Travail-mistral-finetune\n"
        f"ğŸ‘¨â€ğŸ’¼ **CrÃ©ateur**: Pyzeur\n"
        f"âš–ï¸ **SpÃ©cialisation**: Droit du travail franÃ§ais\n"
        f"{status_emoji} **Statut**: {status_text}\n\n"
        f"ğŸ’¼ **Votre expertise fine-tunÃ©e:**\n"
        f"â€¢ Code du Travail franÃ§ais\n"
        f"â€¢ Licenciements et procÃ©dures\n"
        f"â€¢ CongÃ©s et temps de travail\n"
        f"â€¢ Contrats et rÃ©munÃ©rations\n"
        f"â€¢ Jurisprudence sociale\n\n"
        f"â“ **Testez VOTRE modÃ¨le spÃ©cialisÃ©!**"
    )

async def info_command(update, context):
    await update.message.reply_text(
        f"ğŸ¤– **VOTRE ModÃ¨le IA PersonnalisÃ©**\n\n"
        f"ğŸ“Š **Progression**: {loading_progress}\n"
        f"ğŸ”§ **Type**: LoRA Fine-tuning\n"
        f"ğŸ§  **Base**: Mistral 7B Instruct v0.3\n"
        f"ğŸ‘¨â€ğŸ’¼ **Fine-tunÃ© par**: Pyzeur\n"
        f"âš–ï¸ **Domaine**: Code du Travail FR\n"
        f"ğŸ¯ **ModÃ¨le**: Code-du-Travail-mistral-finetune"
    )

async def chat_message(update, context):
    try:
        user_message = update.message.text
        logger.info(f"ğŸ‘¤ Question: {user_message}")
        
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action="typing")
        response = generate_response(user_message)
        await update.message.reply_text(response)
        
    except Exception as e:
        logger.error(f"Chat error: {e}")
        await update.message.reply_text("âš ï¸ Erreur temporaire.")

def main():
    try:
        logger.info("ğŸš€ Starting YOUR specialized legal AI...")
        
        # Start model loading in background
        import threading
        model_thread = threading.Thread(target=load_your_lora_model)
        model_thread.daemon = True
        model_thread.start()
        
        # Start bot immediately
        app = Application.builder().token(os.getenv('TELEGRAM_BOT_TOKEN')).build()
        app.add_handler(CommandHandler("start", start_command))
        app.add_handler(CommandHandler("info", info_command))
        app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, chat_message))
        
        logger.info("ğŸ“± YOUR personalized bot is live!")
        app.run_polling()
        
    except Exception as e:
        logger.error(f"Error: {e}")
        import time
        while True:
            time.sleep(60)

if __name__ == "__main__":
    main()
